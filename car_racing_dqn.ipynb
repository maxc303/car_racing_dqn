{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "#env = gym.make(\"CarRacing-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"
     ]
    }
   ],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n",
    "# Racing Car a\n",
    "VALID_CR_ACTIONS = np.arange(0,16)\n",
    "print(VALID_CR_ACTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2act(num):\n",
    "    '''\n",
    "    convert action index to action input for racing-car-v0\n",
    "    \n",
    "    '''\n",
    "    steer = 0.0\n",
    "    gas = 0.0\n",
    "    brake = 0.0\n",
    "    if(num<9):\n",
    "        steer = (num-4)/4\n",
    "    if(num>=9 and num<13):\n",
    "        gas = (num - 9)/3\n",
    "    if(num>12):\n",
    "        brake = (num-12)/3\n",
    "        \n",
    "    return [steer,gas,brake]\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CR_StateProcessor():\n",
    "    def __init__(self):\n",
    "    # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[96, 96, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 0, 6, 84, 84)\n",
    "#             self.output = tf.image.resize_images(\n",
    "#                 self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1043..1314 -> 271-tiles track\n",
      "[[176 176 176 ... 162 162 162]\n",
      " [176 176 176 ... 162 162 162]\n",
      " [176 176 176 ... 162 162 162]\n",
      " ...\n",
      " [162 162 162 ... 176 176 176]\n",
      " [162 162 162 ... 176 176 176]\n",
      " [162 162 162 ... 176 176 176]]\n",
      "(84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v0\")\n",
    "sp = CR_StateProcessor()\n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    #observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    print(observation_p)\n",
    "    print(observation_p.shape)\n",
    "    \n",
    "    plt.imshow(observation_p)\n",
    "    plt.savefig(\"test.jpeg\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 grayscale frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-49f3f6294684>:33: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-49f3f6294684>:59: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From /home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03024367 0.04143712 0.         0.        ]\n",
      " [0.03024367 0.04143712 0.         0.        ]]\n",
      "99.58649\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 303 (303) @ Episode 1/10000, loss: 0.0012031961232423782\n",
      "Episode Reward: 2.0\n",
      "Step 277 (580) @ Episode 2/10000, loss: 0.00183233304414898166\n",
      "Episode Reward: 2.0\n",
      "Step 238 (818) @ Episode 3/10000, loss: 0.0319921597838401898\n",
      "Episode Reward: 1.0\n",
      "Step 243 (1061) @ Episode 4/10000, loss: 0.0012992125703021884\n",
      "Episode Reward: 2.0\n",
      "Step 314 (1375) @ Episode 5/10000, loss: 0.03312629461288452585\n",
      "Episode Reward: 3.0\n",
      "Step 416 (1791) @ Episode 6/10000, loss: 0.00116313667967915535\n",
      "Episode Reward: 5.0\n",
      "Step 288 (2079) @ Episode 7/10000, loss: 0.0010290199425071478\n",
      "Episode Reward: 2.0\n",
      "Step 175 (2254) @ Episode 8/10000, loss: 0.0016044666990637782\n",
      "Episode Reward: 0.0\n",
      "Step 243 (2497) @ Episode 9/10000, loss: 0.0011176232947036624\n",
      "Episode Reward: 1.0\n",
      "Step 175 (2672) @ Episode 10/10000, loss: 0.0014246655628085136\n",
      "Episode Reward: 0.0\n",
      "Step 313 (2985) @ Episode 11/10000, loss: 0.0023891534656286245\n",
      "Episode Reward: 2.0\n",
      "Step 244 (3229) @ Episode 12/10000, loss: 0.0021652565337717533\n",
      "Episode Reward: 1.0\n",
      "Step 274 (3503) @ Episode 13/10000, loss: 0.0013048711698502302\n",
      "Episode Reward: 2.0\n",
      "Step 171 (3674) @ Episode 14/10000, loss: 0.0017235997365787625\n",
      "Episode Reward: 0.0\n",
      "Step 236 (3910) @ Episode 15/10000, loss: 0.0011184646282345057\n",
      "Episode Reward: 2.0\n",
      "Step 206 (4116) @ Episode 16/10000, loss: 0.0014342801878228784\n",
      "Episode Reward: 1.0\n",
      "Step 274 (4390) @ Episode 17/10000, loss: 0.00198831292800605354\n",
      "Episode Reward: 2.0\n",
      "Step 232 (4622) @ Episode 18/10000, loss: 0.00129532907158136375\n",
      "Episode Reward: 1.0\n",
      "Step 182 (4804) @ Episode 19/10000, loss: 0.0021523311734199524\n",
      "Episode Reward: 0.0\n",
      "Step 235 (5039) @ Episode 20/10000, loss: 0.0011060670949518686\n",
      "Episode Reward: 1.0\n",
      "Step 174 (5213) @ Episode 21/10000, loss: 0.0007315340917557478\n",
      "Episode Reward: 0.0\n",
      "Step 174 (5387) @ Episode 22/10000, loss: 0.0006119199097156525\n",
      "Episode Reward: 0.0\n",
      "Step 211 (5598) @ Episode 23/10000, loss: 0.0016533285379409794\n",
      "Episode Reward: 1.0\n",
      "Step 287 (5885) @ Episode 24/10000, loss: 0.0018864416051656008\n",
      "Episode Reward: 2.0\n",
      "Step 209 (6094) @ Episode 25/10000, loss: 0.0326822772622108466\n",
      "Episode Reward: 1.0\n",
      "Step 291 (6385) @ Episode 26/10000, loss: 0.00122604845091700556\n",
      "Episode Reward: 2.0\n",
      "Step 172 (6557) @ Episode 27/10000, loss: 0.0016868349630385637\n",
      "Episode Reward: 0.0\n",
      "Step 223 (6780) @ Episode 28/10000, loss: 0.00135510379914194353\n",
      "Episode Reward: 1.0\n",
      "Step 333 (7113) @ Episode 29/10000, loss: 0.0014862362295389175\n",
      "Episode Reward: 3.0\n",
      "Step 179 (7292) @ Episode 30/10000, loss: 0.0015371474437415625\n",
      "Episode Reward: 0.0\n",
      "Step 281 (7573) @ Episode 31/10000, loss: 0.00148341478779912266\n",
      "Episode Reward: 2.0\n",
      "Step 166 (7739) @ Episode 32/10000, loss: 0.0014942695852369079\n",
      "Episode Reward: 0.0\n",
      "Step 177 (7916) @ Episode 33/10000, loss: 0.0320113189518451747\n",
      "Episode Reward: 0.0\n",
      "Step 157 (8073) @ Episode 34/10000, loss: 0.0013426874065771722\n",
      "Episode Reward: 0.0\n",
      "Step 166 (8239) @ Episode 35/10000, loss: 0.0329625792801380164\n",
      "Episode Reward: 0.0\n",
      "Step 159 (8398) @ Episode 36/10000, loss: 0.0021340146195143463\n",
      "Episode Reward: 0.0\n",
      "Step 260 (8658) @ Episode 37/10000, loss: 0.0011119665578007698\n",
      "Episode Reward: 1.0\n",
      "Step 234 (8892) @ Episode 38/10000, loss: 0.0005848857108503589\n",
      "Episode Reward: 1.0\n",
      "Step 198 (9090) @ Episode 39/10000, loss: 0.0019001010805368423\n",
      "Episode Reward: 0.0\n",
      "Step 334 (9424) @ Episode 40/10000, loss: 0.00179988541640341285\n",
      "Episode Reward: 3.0\n",
      "Step 227 (9651) @ Episode 41/10000, loss: 0.0331304073333740292\n",
      "Episode Reward: 1.0\n",
      "Step 232 (9883) @ Episode 42/10000, loss: 0.0013136754278093576\n",
      "Episode Reward: 1.0\n",
      "Step 116 (9999) @ Episode 43/10000, loss: 0.0027500109281390905\n",
      "Copied model parameters to target network.\n",
      "Step 178 (10061) @ Episode 43/10000, loss: 0.0013843243941664696\n",
      "Episode Reward: 0.0\n",
      "Step 178 (10239) @ Episode 44/10000, loss: 0.0025234792847186327\n",
      "Episode Reward: 0.0\n",
      "Step 169 (10408) @ Episode 45/10000, loss: 0.0015552287222817548\n",
      "Episode Reward: 0.0\n",
      "Step 407 (10815) @ Episode 46/10000, loss: 0.00191859179176390172\n",
      "Episode Reward: 4.0\n",
      "Step 174 (10989) @ Episode 47/10000, loss: 0.0025365129113197327\n",
      "Episode Reward: 0.0\n",
      "Step 191 (11180) @ Episode 48/10000, loss: 0.0021063773892819888\n",
      "Episode Reward: 0.0\n",
      "Step 174 (11354) @ Episode 49/10000, loss: 0.0009771611075848342\n",
      "Episode Reward: 0.0\n",
      "Step 298 (11652) @ Episode 50/10000, loss: 0.0017719031311571598\n",
      "Episode Reward: 2.0\n",
      "Step 371 (12023) @ Episode 51/10000, loss: 0.00157271709758788357\n",
      "Episode Reward: 3.0\n",
      "Step 241 (12264) @ Episode 52/10000, loss: 0.0020314417779445655\n",
      "Episode Reward: 1.0\n",
      "Step 186 (12450) @ Episode 53/10000, loss: 0.0015556679572910073\n",
      "Episode Reward: 0.0\n",
      "Step 303 (12753) @ Episode 54/10000, loss: 0.0009769131429493427\n",
      "Episode Reward: 2.0\n",
      "Step 195 (12948) @ Episode 55/10000, loss: 0.0016153417527675629\n",
      "Episode Reward: 0.0\n",
      "Step 179 (13127) @ Episode 56/10000, loss: 0.0013784382026642564\n",
      "Episode Reward: 0.0\n",
      "Step 209 (13336) @ Episode 57/10000, loss: 0.0385977663099765845\n",
      "Episode Reward: 1.0\n",
      "Step 291 (13627) @ Episode 58/10000, loss: 0.0019477884052321315\n",
      "Episode Reward: 2.0\n",
      "Step 298 (13925) @ Episode 59/10000, loss: 0.0319927334785461452\n",
      "Episode Reward: 2.0\n",
      "Step 272 (14197) @ Episode 60/10000, loss: 0.0012077435385435827\n",
      "Episode Reward: 2.0\n",
      "Step 242 (14439) @ Episode 61/10000, loss: 0.0003907948557753116\n",
      "Episode Reward: 1.0\n",
      "Step 170 (14609) @ Episode 62/10000, loss: 0.0029639778658747673\n",
      "Episode Reward: 0.0\n",
      "Step 228 (14837) @ Episode 63/10000, loss: 0.0011788178235292435\n",
      "Episode Reward: 1.0\n",
      "Step 190 (15027) @ Episode 64/10000, loss: 0.0013396197464317083\n",
      "Episode Reward: 0.0\n",
      "Step 183 (15210) @ Episode 65/10000, loss: 0.0326574109494686137\n",
      "Episode Reward: 0.0\n",
      "Step 189 (15399) @ Episode 66/10000, loss: 0.0015414840308949351\n",
      "Episode Reward: 0.0\n",
      "Step 282 (15681) @ Episode 67/10000, loss: 0.0019335509277880192\n",
      "Episode Reward: 2.0\n",
      "Step 270 (15951) @ Episode 68/10000, loss: 0.0325608104467392542\n",
      "Episode Reward: 2.0\n",
      "Step 177 (16128) @ Episode 69/10000, loss: 0.00118975853547453883\n",
      "Episode Reward: 0.0\n",
      "Step 268 (16396) @ Episode 70/10000, loss: 0.0318541824817657532\n",
      "Episode Reward: 2.0\n",
      "Step 210 (16606) @ Episode 71/10000, loss: 0.0009624414378777146\n",
      "Episode Reward: 1.0\n",
      "Step 277 (16883) @ Episode 72/10000, loss: 0.0009553494164720178\n",
      "Episode Reward: 2.0\n",
      "Step 338 (17221) @ Episode 73/10000, loss: 0.0023609751369804144\n",
      "Episode Reward: 3.0\n",
      "Step 163 (17384) @ Episode 74/10000, loss: 0.0313676111400127444\n",
      "Episode Reward: 0.0\n",
      "Step 245 (17629) @ Episode 75/10000, loss: 0.0015619045589119196\n",
      "Episode Reward: 1.0\n",
      "Step 175 (17804) @ Episode 76/10000, loss: 0.0017680262681096792\n",
      "Episode Reward: 0.0\n",
      "Step 249 (18053) @ Episode 77/10000, loss: 0.0013357473071664572\n",
      "Episode Reward: 1.0\n",
      "Step 305 (18358) @ Episode 78/10000, loss: 0.0015192055143415928\n",
      "Episode Reward: 2.0\n",
      "Step 182 (18540) @ Episode 79/10000, loss: 0.0019652112387120724\n",
      "Episode Reward: 0.0\n",
      "Step 169 (18709) @ Episode 80/10000, loss: 0.0019511830760166059\n",
      "Episode Reward: 0.0\n",
      "Step 278 (18987) @ Episode 81/10000, loss: 0.06693018972873688945\n",
      "Episode Reward: 2.0\n",
      "Step 230 (19217) @ Episode 82/10000, loss: 0.0015913152601569893\n",
      "Episode Reward: 1.0\n",
      "Step 275 (19492) @ Episode 83/10000, loss: 0.00194963195826858285\n",
      "Episode Reward: 2.0\n",
      "Step 302 (19794) @ Episode 84/10000, loss: 0.0309147257357835777\n",
      "Episode Reward: 2.0\n",
      "Step 191 (19985) @ Episode 85/10000, loss: 0.0016177588840946555\n",
      "Episode Reward: 0.0\n",
      "Step 14 (19999) @ Episode 86/10000, loss: 0.0013638734817504883\n",
      "Copied model parameters to target network.\n",
      "Step 228 (20213) @ Episode 86/10000, loss: 0.0032484836410731077\n",
      "Episode Reward: 1.0\n",
      "Step 191 (20404) @ Episode 87/10000, loss: 0.0014240471646189699\n",
      "Episode Reward: 0.0\n",
      "Step 265 (20669) @ Episode 88/10000, loss: 0.0030849666800349954\n",
      "Episode Reward: 2.0\n",
      "Step 210 (20879) @ Episode 89/10000, loss: 0.0023273874539881945\n",
      "Episode Reward: 1.0\n",
      "Step 273 (21152) @ Episode 90/10000, loss: 0.0318834111094474813\n",
      "Episode Reward: 2.0\n",
      "Step 397 (21549) @ Episode 91/10000, loss: 0.00111318891867995267\n",
      "Episode Reward: 4.0\n",
      "Step 175 (21724) @ Episode 92/10000, loss: 0.0025249416939914227\n",
      "Episode Reward: 0.0\n",
      "Step 237 (21961) @ Episode 93/10000, loss: 0.0024206163361668587\n",
      "Episode Reward: 1.0\n",
      "Step 202 (22163) @ Episode 94/10000, loss: 0.0028295163065195084\n",
      "Episode Reward: 1.0\n",
      "Step 170 (22333) @ Episode 95/10000, loss: 0.0032052402384579182\n",
      "Episode Reward: 0.0\n",
      "Step 162 (22495) @ Episode 96/10000, loss: 0.0021413525100797415\n",
      "Episode Reward: 0.0\n",
      "Step 174 (22669) @ Episode 97/10000, loss: 0.0023315870203077793\n",
      "Episode Reward: 0.0\n",
      "Step 172 (22841) @ Episode 98/10000, loss: 0.0026255042757838964\n",
      "Episode Reward: 0.0\n",
      "Step 178 (23019) @ Episode 99/10000, loss: 0.0011991118080914023\n",
      "Episode Reward: 0.0\n",
      "Step 231 (23250) @ Episode 100/10000, loss: 0.0382968932390213852\n",
      "Episode Reward: 1.0\n",
      "Step 390 (23640) @ Episode 101/10000, loss: 0.0017927445005625486\n",
      "Episode Reward: 4.0\n",
      "Step 220 (23860) @ Episode 102/10000, loss: 0.0018189619295299053\n",
      "Episode Reward: 1.0\n",
      "Step 176 (24036) @ Episode 103/10000, loss: 0.0382175855338573462\n",
      "Episode Reward: 0.0\n",
      "Step 292 (24328) @ Episode 104/10000, loss: 0.0034063672646880155\n",
      "Episode Reward: 2.0\n",
      "Step 244 (24572) @ Episode 105/10000, loss: 0.0029872450977563865\n",
      "Episode Reward: 1.0\n",
      "Step 175 (24747) @ Episode 106/10000, loss: 0.0017219210276380181\n",
      "Episode Reward: 0.0\n",
      "Step 172 (24919) @ Episode 107/10000, loss: 0.0012120623141527176\n",
      "Episode Reward: 0.0\n",
      "Step 232 (25151) @ Episode 108/10000, loss: 0.00178280530963093043\n",
      "Episode Reward: 1.0\n",
      "Step 175 (25326) @ Episode 109/10000, loss: 0.0021854685619473457\n",
      "Episode Reward: 0.0\n",
      "Step 170 (25496) @ Episode 110/10000, loss: 0.0018176197772845626\n",
      "Episode Reward: 0.0\n",
      "Step 208 (25704) @ Episode 111/10000, loss: 0.0024864459410309797\n",
      "Episode Reward: 1.0\n",
      "Step 181 (25885) @ Episode 112/10000, loss: 0.00204136222600936975\n",
      "Episode Reward: 0.0\n",
      "Step 179 (26064) @ Episode 113/10000, loss: 0.0018993685953319073\n",
      "Episode Reward: 0.0\n",
      "Step 293 (26357) @ Episode 114/10000, loss: 0.0298018418252468156\n",
      "Episode Reward: 2.0\n",
      "Step 402 (26759) @ Episode 115/10000, loss: 0.0022544453386217356\n",
      "Episode Reward: 4.0\n",
      "Step 276 (27035) @ Episode 116/10000, loss: 0.0027524661272764206\n",
      "Episode Reward: 2.0\n",
      "Step 321 (27356) @ Episode 117/10000, loss: 0.0394111424684524545\n",
      "Episode Reward: 3.0\n",
      "Step 268 (27624) @ Episode 118/10000, loss: 0.0024740002118051056\n",
      "Episode Reward: 2.0\n",
      "Step 187 (27811) @ Episode 119/10000, loss: 0.0022627385333180428\n",
      "Episode Reward: 0.0\n",
      "Step 309 (28120) @ Episode 120/10000, loss: 0.0014488804154098034\n",
      "Episode Reward: 3.0\n",
      "Step 229 (28349) @ Episode 121/10000, loss: 0.00306178350001573564\n",
      "Episode Reward: 1.0\n",
      "Step 164 (28513) @ Episode 122/10000, loss: 0.00198484235443174845\n",
      "Episode Reward: 0.0\n",
      "Step 213 (28726) @ Episode 123/10000, loss: 0.0283025819808244757\n",
      "Episode Reward: 1.0\n",
      "Step 172 (28898) @ Episode 124/10000, loss: 0.0016369647346436977\n",
      "Episode Reward: 0.0\n",
      "Step 167 (29065) @ Episode 125/10000, loss: 0.0018955969717353582\n",
      "Episode Reward: 0.0\n",
      "Step 240 (29305) @ Episode 126/10000, loss: 0.00175413931719958787\n",
      "Episode Reward: 1.0\n",
      "Step 304 (29609) @ Episode 127/10000, loss: 0.0022492958232760438\n",
      "Episode Reward: 2.0\n",
      "Step 277 (29886) @ Episode 128/10000, loss: 0.0016154341865330935\n",
      "Episode Reward: 2.0\n",
      "Step 113 (29999) @ Episode 129/10000, loss: 0.0025273240171372894\n",
      "Copied model parameters to target network.\n",
      "Step 302 (30188) @ Episode 129/10000, loss: 0.0342170707881450657\n",
      "Episode Reward: 2.0\n",
      "Step 184 (30372) @ Episode 130/10000, loss: 0.0023260510060936213\n",
      "Episode Reward: 0.0\n",
      "Step 170 (30542) @ Episode 131/10000, loss: 0.0436241663992404947\n",
      "Episode Reward: 0.0\n",
      "Step 207 (30749) @ Episode 132/10000, loss: 0.0036524776369333267\n",
      "Episode Reward: 1.0\n",
      "Step 269 (31018) @ Episode 133/10000, loss: 0.0044374344870448116\n",
      "Episode Reward: 2.0\n",
      "Step 356 (31374) @ Episode 134/10000, loss: 0.0019730082713067533\n",
      "Episode Reward: 3.0\n",
      "Step 179 (31553) @ Episode 135/10000, loss: 0.0043423431925475641\n",
      "Episode Reward: 0.0\n",
      "Step 255 (31808) @ Episode 136/10000, loss: 0.0042614373378455647\n",
      "Episode Reward: 2.0\n",
      "Step 176 (31984) @ Episode 137/10000, loss: 0.0029919790104031563\n",
      "Episode Reward: 0.0\n",
      "Step 217 (32201) @ Episode 138/10000, loss: 0.0032394176814705133\n",
      "Episode Reward: 1.0\n",
      "Step 401 (32602) @ Episode 139/10000, loss: 0.0015961152967065573\n",
      "Episode Reward: 4.0\n",
      "Step 169 (32771) @ Episode 140/10000, loss: 0.0028338942211121327\n",
      "Episode Reward: 0.0\n",
      "Step 195 (32966) @ Episode 141/10000, loss: 0.0040286160074174496\n",
      "Episode Reward: 0.0\n",
      "Step 272 (33238) @ Episode 142/10000, loss: 0.0035075496416538954\n",
      "Episode Reward: 2.0\n",
      "Step 324 (33562) @ Episode 143/10000, loss: 0.0015533317346125841\n",
      "Episode Reward: 3.0\n",
      "Step 273 (33835) @ Episode 144/10000, loss: 0.0033932037185877562\n",
      "Episode Reward: 2.0\n",
      "Step 318 (34153) @ Episode 145/10000, loss: 0.0017506496515125036\n",
      "Episode Reward: 3.0\n",
      "Step 170 (34323) @ Episode 146/10000, loss: 0.0031430791132152086\n",
      "Episode Reward: 0.0\n",
      "Step 336 (34659) @ Episode 147/10000, loss: 0.0033459353726357224\n",
      "Episode Reward: 3.0\n",
      "Step 176 (34835) @ Episode 148/10000, loss: 0.0033025518059730536\n",
      "Episode Reward: 0.0\n",
      "Step 176 (35011) @ Episode 149/10000, loss: 0.0030369840096682314\n",
      "Episode Reward: 0.0\n",
      "Step 270 (35281) @ Episode 150/10000, loss: 0.0117686726152896887\n",
      "Episode Reward: 2.0\n",
      "Step 243 (35524) @ Episode 151/10000, loss: 0.0033415898215025663\n",
      "Episode Reward: 1.0\n",
      "Step 174 (35698) @ Episode 152/10000, loss: 0.0237536709755659166\n",
      "Episode Reward: 0.0\n",
      "Step 240 (35938) @ Episode 153/10000, loss: 0.0047118053771555422\n",
      "Episode Reward: 1.0\n",
      "Step 231 (36169) @ Episode 154/10000, loss: 0.0045896600931882866\n",
      "Episode Reward: 1.0\n",
      "Step 214 (36383) @ Episode 155/10000, loss: 0.0034257450606673956\n",
      "Episode Reward: 1.0\n",
      "Step 223 (36606) @ Episode 156/10000, loss: 0.0026948486920446157\n",
      "Episode Reward: 1.0\n",
      "Step 183 (36789) @ Episode 157/10000, loss: 0.0037861897144466644\n",
      "Episode Reward: 0.0\n",
      "Step 184 (36973) @ Episode 158/10000, loss: 0.0032455252949148417\n",
      "Episode Reward: 0.0\n",
      "Step 315 (37288) @ Episode 159/10000, loss: 0.0483919158577919876\n",
      "Episode Reward: 3.0\n",
      "Step 163 (37451) @ Episode 160/10000, loss: 0.0034728334285318857\n",
      "Episode Reward: 0.0\n",
      "Step 176 (37627) @ Episode 161/10000, loss: 0.0049355113878846177\n",
      "Episode Reward: 0.0\n",
      "Step 237 (37864) @ Episode 162/10000, loss: 0.0023123014252632856\n",
      "Episode Reward: 1.0\n",
      "Step 202 (38066) @ Episode 163/10000, loss: 0.0025093127042055138\n",
      "Episode Reward: 0.0\n",
      "Step 185 (38251) @ Episode 164/10000, loss: 0.0020117517560720444\n",
      "Episode Reward: 0.0\n",
      "Step 176 (38427) @ Episode 165/10000, loss: 0.0172566957771778128\n",
      "Episode Reward: 0.0\n",
      "Step 286 (38713) @ Episode 166/10000, loss: 0.0016642464324831963\n",
      "Episode Reward: 2.0\n",
      "Step 172 (38885) @ Episode 167/10000, loss: 0.0029306961223483086\n",
      "Episode Reward: 0.0\n",
      "Step 180 (39065) @ Episode 168/10000, loss: 0.0034948736429214478\n",
      "Episode Reward: 0.0\n",
      "Step 265 (39330) @ Episode 169/10000, loss: 0.0029907436110079294\n",
      "Episode Reward: 2.0\n",
      "Step 169 (39499) @ Episode 170/10000, loss: 0.0042013833299279212\n",
      "Episode Reward: 0.0\n",
      "Step 427 (39926) @ Episode 171/10000, loss: 0.0027006110176444054\n",
      "Episode Reward: 7.0\n",
      "Step 73 (39999) @ Episode 172/10000, loss: 0.0026689257938414818\n",
      "Copied model parameters to target network.\n",
      "Step 165 (40091) @ Episode 172/10000, loss: 0.0050326511263847356\n",
      "Episode Reward: 0.0\n",
      "Step 214 (40305) @ Episode 173/10000, loss: 0.0022899210453033447\n",
      "Episode Reward: 1.0\n",
      "Step 338 (40643) @ Episode 174/10000, loss: 0.0015123699558898807\n",
      "Episode Reward: 3.0\n",
      "Step 176 (40819) @ Episode 175/10000, loss: 0.0021314914338290695\n",
      "Episode Reward: 0.0\n",
      "Step 287 (41106) @ Episode 176/10000, loss: 0.0075520365498960024\n",
      "Episode Reward: 2.0\n",
      "Step 178 (41284) @ Episode 177/10000, loss: 0.0168160311877727563\n",
      "Episode Reward: 0.0\n",
      "Step 173 (41457) @ Episode 178/10000, loss: 0.0018181258346885443\n",
      "Episode Reward: 0.0\n",
      "Step 180 (41637) @ Episode 179/10000, loss: 0.0023856956977397203\n",
      "Episode Reward: 0.0\n",
      "Step 228 (41865) @ Episode 180/10000, loss: 0.00185958202928304676\n",
      "Episode Reward: 1.0\n",
      "Step 275 (42140) @ Episode 181/10000, loss: 0.0025245840661227703\n",
      "Episode Reward: 2.0\n",
      "Step 431 (42571) @ Episode 182/10000, loss: 0.0025897226296365265\n",
      "Episode Reward: 5.0\n",
      "Step 179 (42750) @ Episode 183/10000, loss: 0.0472140125930309308\n",
      "Episode Reward: 0.0\n",
      "Step 171 (42921) @ Episode 184/10000, loss: 0.0021940986625850245\n",
      "Episode Reward: 0.0\n",
      "Step 268 (43189) @ Episode 185/10000, loss: 0.0033103751484304667\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 260 (43449) @ Episode 186/10000, loss: 0.0030880649574100977\n",
      "Episode Reward: 2.0\n",
      "Step 169 (43618) @ Episode 187/10000, loss: 0.0028221546672284603\n",
      "Episode Reward: 0.0\n",
      "Step 170 (43788) @ Episode 188/10000, loss: 0.0023528144229203463\n",
      "Episode Reward: 0.0\n",
      "Step 506 (44294) @ Episode 189/10000, loss: 0.0027646133676171303\n",
      "Episode Reward: 5.0\n",
      "Step 355 (44649) @ Episode 190/10000, loss: 0.0011241361498832703\n",
      "Episode Reward: 3.0\n",
      "Step 176 (44825) @ Episode 191/10000, loss: 0.00225990940816700465\n",
      "Episode Reward: 0.0\n",
      "Step 242 (45067) @ Episode 192/10000, loss: 0.0124571621417999273\n",
      "Episode Reward: 1.0\n",
      "Step 336 (45403) @ Episode 193/10000, loss: 0.00311035919003188677\n",
      "Episode Reward: 3.0\n",
      "Step 372 (45775) @ Episode 194/10000, loss: 0.0097535979002714165\n",
      "Episode Reward: 4.0\n",
      "Step 286 (46061) @ Episode 195/10000, loss: 0.0423781238496303565\n",
      "Episode Reward: 2.0\n",
      "Step 278 (46339) @ Episode 196/10000, loss: 0.0024539260193705568\n",
      "Episode Reward: 2.0\n",
      "Step 358 (46697) @ Episode 197/10000, loss: 0.0418542921543121343\n",
      "Episode Reward: 3.0\n",
      "Step 182 (46879) @ Episode 198/10000, loss: 0.0023740590550005436\n",
      "Episode Reward: 0.0\n",
      "Step 243 (47122) @ Episode 199/10000, loss: 0.0037299422547221184\n",
      "Episode Reward: 1.0\n",
      "Step 178 (47300) @ Episode 200/10000, loss: 0.0028679352253675463\n",
      "Episode Reward: 0.0\n",
      "Step 230 (47530) @ Episode 201/10000, loss: 0.0053920848295092585\n",
      "Episode Reward: 1.0\n",
      "Step 277 (47807) @ Episode 202/10000, loss: 0.0432941243052482694\n",
      "Episode Reward: 2.0\n",
      "Step 414 (48221) @ Episode 203/10000, loss: 0.0021359135862439875\n",
      "Episode Reward: 4.0\n",
      "Step 272 (48493) @ Episode 204/10000, loss: 0.0010716363321989775\n",
      "Episode Reward: 2.0\n",
      "Step 159 (48652) @ Episode 205/10000, loss: 0.0035639717243611813\n",
      "Episode Reward: 0.0\n",
      "Step 161 (48813) @ Episode 206/10000, loss: 0.0026052854955196386\n",
      "Episode Reward: 0.0\n",
      "Step 280 (49093) @ Episode 207/10000, loss: 0.0025008202064782382\n",
      "Episode Reward: 2.0\n",
      "Step 249 (49342) @ Episode 208/10000, loss: 0.00208233390003442766\n",
      "Episode Reward: 2.0\n",
      "Step 222 (49564) @ Episode 209/10000, loss: 0.0026827533729374417\n",
      "Episode Reward: 1.0\n",
      "Step 353 (49917) @ Episode 210/10000, loss: 0.00222499738447368144\n",
      "Episode Reward: 3.0\n",
      "Step 82 (49999) @ Episode 211/10000, loss: 0.0019414660055190325\n",
      "Copied model parameters to target network.\n",
      "Step 300 (50217) @ Episode 211/10000, loss: 0.0035555879585444927\n",
      "Episode Reward: 2.0\n",
      "Step 383 (50600) @ Episode 212/10000, loss: 0.0010991181479766965\n",
      "Episode Reward: 4.0\n",
      "Step 285 (50885) @ Episode 213/10000, loss: 0.0082049518823623664\n",
      "Episode Reward: 2.0\n",
      "Step 170 (51055) @ Episode 214/10000, loss: 0.0272380337119102486\n",
      "Episode Reward: 0.0\n",
      "Step 200 (51255) @ Episode 215/10000, loss: 0.0041834739968180662\n",
      "Episode Reward: 1.0\n",
      "Step 241 (51496) @ Episode 216/10000, loss: 0.00017318135360255837\n",
      "Episode Reward: 1.0\n",
      "Step 238 (51734) @ Episode 217/10000, loss: 0.00025813930551521487\n",
      "Episode Reward: 1.0\n",
      "Step 364 (52098) @ Episode 218/10000, loss: 0.00049848493654280982\n",
      "Episode Reward: 3.0\n",
      "Step 334 (52432) @ Episode 219/10000, loss: 0.00033694170997478077\n",
      "Episode Reward: 3.0\n",
      "Step 236 (52668) @ Episode 220/10000, loss: 0.00199584639631211764\n",
      "Episode Reward: 1.0\n",
      "Step 177 (52845) @ Episode 221/10000, loss: 0.00028146762633696255\n",
      "Episode Reward: 0.0\n",
      "Step 256 (53101) @ Episode 222/10000, loss: 0.00016982883971650153\n",
      "Episode Reward: 2.0\n",
      "Step 396 (53497) @ Episode 223/10000, loss: 9.586162923369557e-052\n",
      "Episode Reward: 3.0\n",
      "Step 282 (53779) @ Episode 224/10000, loss: 0.00471570109948515953\n",
      "Episode Reward: 2.0\n",
      "Step 416 (54195) @ Episode 225/10000, loss: 8.50525830173865e-0574\n",
      "Episode Reward: 5.0\n",
      "Step 318 (54513) @ Episode 226/10000, loss: 0.00036613136762753135\n",
      "Episode Reward: 3.0\n",
      "Step 246 (54759) @ Episode 227/10000, loss: 0.00013042759383097298\n",
      "Episode Reward: 1.0\n",
      "Step 235 (54994) @ Episode 228/10000, loss: 0.00037498533492907885\n",
      "Episode Reward: 1.0\n",
      "Step 186 (55180) @ Episode 229/10000, loss: 5.369298014556989e-058\n",
      "Episode Reward: 0.0\n",
      "Step 183 (55363) @ Episode 230/10000, loss: 0.00108509953133761887\n",
      "Episode Reward: 0.0\n",
      "Step 178 (55541) @ Episode 231/10000, loss: 9.052627865457907e-055\n",
      "Episode Reward: 0.0\n",
      "Step 278 (55819) @ Episode 232/10000, loss: 0.00051669054664671426\n",
      "Episode Reward: 2.0\n",
      "Step 273 (56092) @ Episode 233/10000, loss: 3.5429220588412136e-05\n",
      "Episode Reward: 2.0\n",
      "Step 185 (56277) @ Episode 234/10000, loss: 0.00102896988391876225\n",
      "Episode Reward: 0.0\n",
      "Step 172 (56449) @ Episode 235/10000, loss: 0.00104930123779922727\n",
      "Episode Reward: 0.0\n",
      "Step 238 (56687) @ Episode 236/10000, loss: 0.00019253852951806039\n",
      "Episode Reward: 1.0\n",
      "Step 239 (56926) @ Episode 237/10000, loss: 5.627366772387177e-055\n",
      "Episode Reward: 1.0\n",
      "Step 268 (57194) @ Episode 238/10000, loss: 0.00044637732207775116\n",
      "Episode Reward: 2.0\n",
      "Step 218 (57412) @ Episode 239/10000, loss: 0.00233705574646592144\n",
      "Episode Reward: 1.0\n",
      "Step 311 (57723) @ Episode 240/10000, loss: 0.00891798920929432266\n",
      "Episode Reward: 2.0\n",
      "Step 279 (58002) @ Episode 241/10000, loss: 0.00011119752161903307\n",
      "Episode Reward: 2.0\n",
      "Step 198 (58200) @ Episode 242/10000, loss: 0.00118355860468000172\n",
      "Episode Reward: 0.0\n",
      "Step 231 (58431) @ Episode 243/10000, loss: 0.00011412559251766652\n",
      "Episode Reward: 1.0\n",
      "Step 191 (58622) @ Episode 244/10000, loss: 0.00087040395010262736\n",
      "Episode Reward: 0.0\n",
      "Step 261 (58883) @ Episode 245/10000, loss: 0.00030903008882887667\n",
      "Episode Reward: 2.0\n",
      "Step 231 (59114) @ Episode 246/10000, loss: 0.00033569321385584775\n",
      "Episode Reward: 1.0\n",
      "Step 177 (59291) @ Episode 247/10000, loss: 0.00375755806453526475\n",
      "Episode Reward: 0.0\n",
      "Step 185 (59476) @ Episode 248/10000, loss: 0.00020509822934400295\n",
      "Episode Reward: 0.0\n",
      "Step 180 (59656) @ Episode 249/10000, loss: 0.00013866972585674375\n",
      "Episode Reward: 0.0\n",
      "Step 162 (59818) @ Episode 250/10000, loss: 0.00040703537524677813\n",
      "Episode Reward: 0.0\n",
      "Step 181 (59999) @ Episode 251/10000, loss: 0.00014337443280965095\n",
      "Copied model parameters to target network.\n",
      "Step 223 (60041) @ Episode 251/10000, loss: 0.00040149257984012365\n",
      "Episode Reward: 1.0\n",
      "Step 224 (60265) @ Episode 252/10000, loss: 0.00019366209744475782\n",
      "Episode Reward: 1.0\n",
      "Step 199 (60464) @ Episode 253/10000, loss: 7.7668039011769e-05895\n",
      "Episode Reward: 0.0\n",
      "Step 372 (60836) @ Episode 254/10000, loss: 0.00452487170696258548\n",
      "Episode Reward: 3.0\n",
      "Step 207 (61043) @ Episode 255/10000, loss: 0.00056423200294375425\n",
      "Episode Reward: 1.0\n",
      "Step 193 (61236) @ Episode 256/10000, loss: 0.03460520133376121575\n",
      "Episode Reward: 0.0\n",
      "Step 172 (61408) @ Episode 257/10000, loss: 0.00196783733554184442\n",
      "Episode Reward: 0.0\n",
      "Step 172 (61580) @ Episode 258/10000, loss: 3.376324457349256e-053\n",
      "Episode Reward: 0.0\n",
      "Step 258 (61838) @ Episode 259/10000, loss: 0.00243572657927870755\n",
      "Episode Reward: 1.0\n",
      "Step 171 (62009) @ Episode 260/10000, loss: 0.00016125333786476403\n",
      "Episode Reward: 0.0\n",
      "Step 187 (62196) @ Episode 261/10000, loss: 0.00156074890401214365\n",
      "Episode Reward: 0.0\n",
      "Step 419 (62615) @ Episode 262/10000, loss: 0.00477339001372456554\n",
      "Episode Reward: 4.0\n",
      "Step 177 (62792) @ Episode 263/10000, loss: 0.02127728797495365535\n",
      "Episode Reward: 0.0\n",
      "Step 233 (63025) @ Episode 264/10000, loss: 0.00050293747335672388\n",
      "Episode Reward: 1.0\n",
      "Step 339 (63364) @ Episode 265/10000, loss: 0.00014339303015731275\n",
      "Episode Reward: 3.0\n",
      "Step 184 (63548) @ Episode 266/10000, loss: 0.00035622372524812823\n",
      "Episode Reward: 0.0\n",
      "Step 210 (63758) @ Episode 267/10000, loss: 2.3709129891358316e-05\n",
      "Episode Reward: 1.0\n",
      "Step 166 (63924) @ Episode 268/10000, loss: 0.00567707326263189387\n",
      "Episode Reward: 0.0\n",
      "Step 293 (64217) @ Episode 269/10000, loss: 0.00039503924199379983\n",
      "Episode Reward: 2.0\n",
      "Step 172 (64389) @ Episode 270/10000, loss: 0.00053768645739182835\n",
      "Episode Reward: 0.0\n",
      "Step 390 (64779) @ Episode 271/10000, loss: 0.00422902405261993473\n",
      "Episode Reward: 4.0\n",
      "Step 246 (65025) @ Episode 272/10000, loss: 0.00024619061150588095\n",
      "Episode Reward: 1.0\n",
      "Step 240 (65265) @ Episode 273/10000, loss: 2.013650009757839e-055\n",
      "Episode Reward: 1.0\n",
      "Step 174 (65439) @ Episode 274/10000, loss: 0.00045568245695903897\n",
      "Episode Reward: 0.0\n",
      "Step 225 (65664) @ Episode 275/10000, loss: 0.00021652397117577493\n",
      "Episode Reward: 1.0\n",
      "Step 284 (65948) @ Episode 276/10000, loss: 0.00014393132005352527\n",
      "Episode Reward: 2.0\n",
      "Step 169 (66117) @ Episode 277/10000, loss: 0.00032613507937639955\n",
      "Episode Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 277 (66394) @ Episode 278/10000, loss: 0.00114595913328230388\n",
      "Episode Reward: 2.0\n",
      "Step 273 (66667) @ Episode 279/10000, loss: 4.578994048642926e-053\n",
      "Episode Reward: 2.0\n",
      "Step 166 (66833) @ Episode 280/10000, loss: 8.396302291657776e-055\n",
      "Episode Reward: 0.0\n",
      "Step 205 (67038) @ Episode 281/10000, loss: 0.00161115219816565517\n",
      "Episode Reward: 1.0\n",
      "Step 392 (67430) @ Episode 282/10000, loss: 0.00035299098817631697\n",
      "Episode Reward: 4.0\n",
      "Step 299 (67729) @ Episode 283/10000, loss: 0.00021008064504712825\n",
      "Episode Reward: 2.0\n",
      "Step 274 (68003) @ Episode 284/10000, loss: 0.00422173226252198247\n",
      "Episode Reward: 2.0\n",
      "Step 233 (68236) @ Episode 285/10000, loss: 0.00120627006981521848\n",
      "Episode Reward: 1.0\n",
      "Step 213 (68449) @ Episode 286/10000, loss: 5.369945938582532e-053\n",
      "Episode Reward: 1.0\n",
      "Step 298 (68747) @ Episode 287/10000, loss: 0.00030031151254661383\n",
      "Episode Reward: 2.0\n",
      "Step 166 (68913) @ Episode 288/10000, loss: 0.00010969638969982043\n",
      "Episode Reward: 0.0\n",
      "Step 182 (69095) @ Episode 289/10000, loss: 0.00204929197207093243\n",
      "Episode Reward: 0.0\n",
      "Step 438 (69533) @ Episode 290/10000, loss: 0.00292448117397725657\n",
      "Episode Reward: 5.0\n",
      "Step 293 (69826) @ Episode 291/10000, loss: 0.00100277503952384135\n",
      "Episode Reward: 2.0\n",
      "Step 173 (69999) @ Episode 292/10000, loss: 4.1058789065573364e-05\n",
      "Copied model parameters to target network.\n",
      "Step 229 (70055) @ Episode 292/10000, loss: 0.00047061374061740935\n",
      "Episode Reward: 1.0\n",
      "Step 227 (70282) @ Episode 293/10000, loss: 0.00015965601778589193\n",
      "Episode Reward: 1.0\n",
      "Step 164 (70446) @ Episode 294/10000, loss: 0.00130062387324869637\n",
      "Episode Reward: 0.0\n",
      "Step 356 (70802) @ Episode 295/10000, loss: 0.00028444683994166553\n",
      "Episode Reward: 3.0\n",
      "Step 175 (70977) @ Episode 296/10000, loss: 0.00177962752059102065\n",
      "Episode Reward: 0.0\n",
      "Step 240 (71217) @ Episode 297/10000, loss: 0.00451060896739363753\n",
      "Episode Reward: 1.0\n",
      "Step 180 (71397) @ Episode 298/10000, loss: 8.881330722942948e-055\n",
      "Episode Reward: 0.0\n",
      "Step 255 (71652) @ Episode 299/10000, loss: 0.00259160762652754834\n",
      "Episode Reward: 1.0\n",
      "Step 229 (71881) @ Episode 300/10000, loss: 0.00196812674403190621\n",
      "Episode Reward: 1.0\n",
      "Step 200 (72081) @ Episode 301/10000, loss: 0.00075865635881200433\n",
      "Episode Reward: 1.0\n",
      "Step 263 (72344) @ Episode 302/10000, loss: 0.00080746482126414784\n",
      "Episode Reward: 2.0\n",
      "Step 291 (72635) @ Episode 303/10000, loss: 0.00013391450920607895\n",
      "Episode Reward: 2.0\n",
      "Step 291 (72926) @ Episode 304/10000, loss: 0.00013054176815785468\n",
      "Episode Reward: 2.0\n",
      "Step 313 (73239) @ Episode 305/10000, loss: 0.00091906363377347594\n",
      "Episode Reward: 3.0\n",
      "Step 245 (73484) @ Episode 306/10000, loss: 0.00028778304113075137\n",
      "Episode Reward: 1.0\n",
      "Step 186 (73670) @ Episode 307/10000, loss: 0.00043525054934434593\n",
      "Episode Reward: 0.0\n",
      "Step 183 (73853) @ Episode 308/10000, loss: 0.00047458807239308953\n",
      "Episode Reward: 0.0\n",
      "Step 314 (74167) @ Episode 309/10000, loss: 0.00112569320481270556\n",
      "Episode Reward: 2.0\n",
      "Step 179 (74346) @ Episode 310/10000, loss: 0.00049647456035017976\n",
      "Episode Reward: 0.0\n",
      "Step 167 (74513) @ Episode 311/10000, loss: 5.757611870649271e-056\n",
      "Episode Reward: 0.0\n",
      "Step 209 (74722) @ Episode 312/10000, loss: 0.00033620020258240472\n",
      "Episode Reward: 1.0\n",
      "Step 282 (75004) @ Episode 313/10000, loss: 6.151571869850159e-054\n",
      "Episode Reward: 2.0\n",
      "Step 180 (75184) @ Episode 314/10000, loss: 0.00036090693902224355\n",
      "Episode Reward: 0.0\n",
      "Step 271 (75455) @ Episode 315/10000, loss: 6.560490146512166e-054\n",
      "Episode Reward: 2.0\n",
      "Step 241 (75696) @ Episode 316/10000, loss: 0.00054335052845999675\n",
      "Episode Reward: 1.0\n",
      "Step 167 (75863) @ Episode 317/10000, loss: 0.00153135473374277357\n",
      "Episode Reward: 0.0\n",
      "Step 409 (76272) @ Episode 318/10000, loss: 0.00197322340682148933\n",
      "Episode Reward: 5.0\n",
      "Step 269 (76541) @ Episode 319/10000, loss: 0.00149992178194224835\n",
      "Episode Reward: 2.0\n",
      "Step 339 (76880) @ Episode 320/10000, loss: 0.00226017739623785872\n",
      "Episode Reward: 3.0\n",
      "Step 167 (77047) @ Episode 321/10000, loss: 0.00018668420671019703\n",
      "Episode Reward: 0.0\n",
      "Step 247 (77294) @ Episode 322/10000, loss: 0.00067248981213197118\n",
      "Episode Reward: 1.0\n",
      "Step 329 (77623) @ Episode 323/10000, loss: 0.00010403087071608752\n",
      "Episode Reward: 3.0\n",
      "Step 310 (77933) @ Episode 324/10000, loss: 0.00015519511362072085\n",
      "Episode Reward: 2.0\n",
      "Step 176 (78109) @ Episode 325/10000, loss: 6.456903793150559e-054\n",
      "Episode Reward: 0.0\n",
      "Step 321 (78430) @ Episode 326/10000, loss: 9.584355575498194e-053\n",
      "Episode Reward: 3.0\n",
      "Step 311 (78741) @ Episode 327/10000, loss: 0.00027291214792057874\n",
      "Episode Reward: 2.0\n",
      "Step 244 (78985) @ Episode 328/10000, loss: 0.00056600134121254098\n",
      "Episode Reward: 1.0\n",
      "Step 312 (79297) @ Episode 329/10000, loss: 3.8606609450653195e-05\n",
      "Episode Reward: 2.0\n",
      "Step 178 (79475) @ Episode 330/10000, loss: 0.00146675412543118434\n",
      "Episode Reward: 0.0\n",
      "Step 247 (79722) @ Episode 331/10000, loss: 0.00233990303240716464\n",
      "Episode Reward: 2.0\n",
      "Step 277 (79999) @ Episode 332/10000, loss: 0.00040186362457461655\n",
      "Copied model parameters to target network.\n",
      "Step 340 (80062) @ Episode 332/10000, loss: 4.287101182853803e-053\n",
      "Episode Reward: 3.0\n",
      "Step 233 (80295) @ Episode 333/10000, loss: 0.00041829765541478993\n",
      "Episode Reward: 1.0\n",
      "Step 176 (80471) @ Episode 334/10000, loss: 0.00137744680978357816\n",
      "Episode Reward: 0.0\n",
      "Step 259 (80730) @ Episode 335/10000, loss: 0.00132098258472979077\n",
      "Episode Reward: 1.0\n",
      "Step 372 (81102) @ Episode 336/10000, loss: 0.00026179582346230745\n",
      "Episode Reward: 4.0\n",
      "Step 313 (81415) @ Episode 337/10000, loss: 9.804638102650642e-057\n",
      "Episode Reward: 2.0\n",
      "Step 167 (81582) @ Episode 338/10000, loss: 0.00098241935484111355\n",
      "Episode Reward: 0.0\n",
      "Step 178 (81760) @ Episode 339/10000, loss: 0.00087333028204739095\n",
      "Episode Reward: 0.0\n",
      "Step 266 (82026) @ Episode 340/10000, loss: 0.00204148562625050547\n",
      "Episode Reward: 2.0\n",
      "Step 229 (82255) @ Episode 341/10000, loss: 0.00023188476916402578\n",
      "Episode Reward: 1.0\n",
      "Step 166 (82421) @ Episode 342/10000, loss: 9.564160427544266e-054\n",
      "Episode Reward: 0.0\n",
      "Step 242 (82663) @ Episode 343/10000, loss: 4.697123949881643e-056\n",
      "Episode Reward: 1.0\n",
      "Step 172 (82835) @ Episode 344/10000, loss: 0.00581222865730524155\n",
      "Episode Reward: 0.0\n",
      "Step 280 (83115) @ Episode 345/10000, loss: 0.00192661408800631762\n",
      "Episode Reward: 2.0\n",
      "Step 171 (83286) @ Episode 346/10000, loss: 0.00011193794489372522\n",
      "Episode Reward: 0.0\n",
      "Step 166 (83452) @ Episode 347/10000, loss: 0.00019667226297315215\n",
      "Episode Reward: 0.0\n",
      "Step 314 (83766) @ Episode 348/10000, loss: 4.1087911085924134e-05\n",
      "Episode Reward: 2.0\n",
      "Step 272 (84038) @ Episode 349/10000, loss: 0.00033911992795765496\n",
      "Episode Reward: 2.0\n",
      "Step 280 (84318) @ Episode 350/10000, loss: 0.00010055511665996164\n",
      "Episode Reward: 2.0\n",
      "Step 274 (84592) @ Episode 351/10000, loss: 6.084236883907579e-053\n",
      "Episode Reward: 2.0\n",
      "Step 358 (84950) @ Episode 352/10000, loss: 0.00076753576286137145\n",
      "Episode Reward: 3.0\n",
      "Step 243 (85193) @ Episode 353/10000, loss: 0.00036182629992254087\n",
      "Episode Reward: 1.0\n",
      "Step 307 (85500) @ Episode 354/10000, loss: 0.00011265505600022152\n",
      "Episode Reward: 2.0\n",
      "Step 327 (85827) @ Episode 355/10000, loss: 0.00013556904741562903\n",
      "Episode Reward: 2.0\n",
      "Step 208 (86035) @ Episode 356/10000, loss: 0.00050528795691207058\n",
      "Episode Reward: 1.0\n",
      "Step 178 (86213) @ Episode 357/10000, loss: 5.492013951879926e-052\n",
      "Episode Reward: 0.0\n",
      "Step 351 (86564) @ Episode 358/10000, loss: 7.849073153920472e-053\n",
      "Episode Reward: 3.0\n",
      "Step 209 (86773) @ Episode 359/10000, loss: 0.00033920560963451861\n",
      "Episode Reward: 1.0\n",
      "Step 174 (86947) @ Episode 360/10000, loss: 0.00056285521714016875\n",
      "Episode Reward: 0.0\n",
      "Step 235 (87182) @ Episode 361/10000, loss: 2.7981986931990832e-05\n",
      "Episode Reward: 1.0\n",
      "Step 368 (87550) @ Episode 362/10000, loss: 1.3770903024123982e-05\n",
      "Episode Reward: 3.0\n",
      "Step 307 (87857) @ Episode 363/10000, loss: 0.00013811755343340337\n",
      "Episode Reward: 3.0\n",
      "Step 183 (88040) @ Episode 364/10000, loss: 0.00032861565705388784\n",
      "Episode Reward: 0.0\n",
      "Step 190 (88230) @ Episode 365/10000, loss: 0.00374660547822713852\n",
      "Episode Reward: 0.0\n",
      "Step 415 (88645) @ Episode 366/10000, loss: 0.00015663269732613117\n",
      "Episode Reward: 4.0\n",
      "Step 271 (88916) @ Episode 367/10000, loss: 0.00136433448642492304\n",
      "Episode Reward: 2.0\n",
      "Step 242 (89158) @ Episode 368/10000, loss: 0.00068460154579952366\n",
      "Episode Reward: 1.0\n",
      "Step 216 (89374) @ Episode 369/10000, loss: 0.00040776407695375383\n",
      "Episode Reward: 1.0\n",
      "Step 193 (89567) @ Episode 370/10000, loss: 0.00088251649867743256\n",
      "Episode Reward: 0.0\n",
      "Step 247 (89814) @ Episode 371/10000, loss: 0.00159689795691519984\n",
      "Episode Reward: 2.0\n",
      "Step 185 (89999) @ Episode 372/10000, loss: 5.916385998716578e-054\n",
      "Copied model parameters to target network.\n",
      "Step 243 (90057) @ Episode 372/10000, loss: 0.00736548285931348877\n",
      "Episode Reward: 1.0\n",
      "Step 270 (90327) @ Episode 373/10000, loss: 0.00127623369917273522\n",
      "Episode Reward: 2.0\n",
      "Step 274 (90601) @ Episode 374/10000, loss: 0.00015285065455827862\n",
      "Episode Reward: 2.0\n",
      "Step 235 (90836) @ Episode 375/10000, loss: 0.00056030985433608295\n",
      "Episode Reward: 1.0\n",
      "Step 233 (91069) @ Episode 376/10000, loss: 0.00259243906475603648\n",
      "Episode Reward: 1.0\n",
      "Step 243 (91312) @ Episode 377/10000, loss: 5.895304275327362e-055\n",
      "Episode Reward: 1.0\n",
      "Step 240 (91552) @ Episode 378/10000, loss: 9.939997107721865e-056\n",
      "Episode Reward: 1.0\n",
      "Step 408 (91960) @ Episode 379/10000, loss: 0.00053546065464615826\n",
      "Episode Reward: 4.0\n",
      "Step 311 (92271) @ Episode 380/10000, loss: 0.00273518892936408538\n",
      "Episode Reward: 2.0\n",
      "Step 221 (92492) @ Episode 381/10000, loss: 0.00051431421888992195\n",
      "Episode Reward: 1.0\n",
      "Step 171 (92663) @ Episode 382/10000, loss: 5.485155270434916e-055\n",
      "Episode Reward: 0.0\n",
      "Step 181 (92844) @ Episode 383/10000, loss: 0.00043236845522187655\n",
      "Episode Reward: 0.0\n",
      "Step 227 (93071) @ Episode 384/10000, loss: 0.00051172036910429645\n",
      "Episode Reward: 1.0\n",
      "Step 243 (93314) @ Episode 385/10000, loss: 0.00140578963328152943\n",
      "Episode Reward: 1.0\n",
      "Step 238 (93552) @ Episode 386/10000, loss: 0.00110255624167621147\n",
      "Episode Reward: 1.0\n",
      "Step 163 (93715) @ Episode 387/10000, loss: 0.00080748659092932943\n",
      "Episode Reward: 0.0\n",
      "Step 211 (93926) @ Episode 388/10000, loss: 9.38742741709575e-0585\n",
      "Episode Reward: 1.0\n",
      "Step 158 (94084) @ Episode 389/10000, loss: 3.8751553802285343e-05\n",
      "Episode Reward: 0.0\n",
      "Step 286 (94370) @ Episode 390/10000, loss: 0.00077492726268246777\n",
      "Episode Reward: 2.0\n",
      "Step 183 (94553) @ Episode 391/10000, loss: 0.00033148966031149035\n",
      "Episode Reward: 0.0\n",
      "Step 274 (94827) @ Episode 392/10000, loss: 0.00311044580303132532\n",
      "Episode Reward: 2.0\n",
      "Step 178 (95005) @ Episode 393/10000, loss: 0.00045108675840310755\n",
      "Episode Reward: 0.0\n",
      "Step 178 (95183) @ Episode 394/10000, loss: 0.00038720326847396798\n",
      "Episode Reward: 0.0\n",
      "Step 233 (95416) @ Episode 395/10000, loss: 0.00013632062473334372\n",
      "Episode Reward: 1.0\n",
      "Step 176 (95592) @ Episode 396/10000, loss: 0.00181950791738927365\n",
      "Episode Reward: 0.0\n",
      "Step 181 (95773) @ Episode 397/10000, loss: 0.00015861933934502304\n",
      "Episode Reward: 0.0\n",
      "Step 235 (96008) @ Episode 398/10000, loss: 9.1500100097619e-05266\n",
      "Episode Reward: 1.0\n",
      "Step 304 (96312) @ Episode 399/10000, loss: 0.00931131653487682305\n",
      "Episode Reward: 2.0\n",
      "Step 186 (96498) @ Episode 400/10000, loss: 0.00044363667257130146\n",
      "Episode Reward: 0.0\n",
      "Step 278 (96776) @ Episode 401/10000, loss: 0.00183452211786061535\n",
      "Episode Reward: 2.0\n",
      "Step 279 (97055) @ Episode 402/10000, loss: 8.149769564624876e-053\n",
      "Episode Reward: 2.0\n",
      "Step 299 (97354) @ Episode 403/10000, loss: 0.00010857394227059558\n",
      "Episode Reward: 2.0\n",
      "Step 360 (97714) @ Episode 404/10000, loss: 0.00069776806049048985\n",
      "Episode Reward: 3.0\n",
      "Step 381 (98095) @ Episode 405/10000, loss: 0.00023880631488282233\n",
      "Episode Reward: 3.0\n",
      "Step 291 (98386) @ Episode 406/10000, loss: 0.00014920139801688492\n",
      "Episode Reward: 2.0\n",
      "Step 182 (98568) @ Episode 407/10000, loss: 0.00039933819789439445\n",
      "Episode Reward: 0.0\n",
      "Step 306 (98874) @ Episode 408/10000, loss: 0.00067870441125705844\n",
      "Episode Reward: 2.0\n",
      "Step 271 (99145) @ Episode 409/10000, loss: 0.00082915875827893617\n",
      "Episode Reward: 2.0\n",
      "Step 181 (99326) @ Episode 410/10000, loss: 4.32625092798844e-0524\n",
      "Episode Reward: 0.0\n",
      "Step 189 (99515) @ Episode 411/10000, loss: 0.00063633016543462873\n",
      "Episode Reward: 0.0\n",
      "Step 291 (99806) @ Episode 412/10000, loss: 0.00022544978128280493\n",
      "Episode Reward: 2.0\n",
      "Step 183 (99989) @ Episode 413/10000, loss: 0.00019289657939225435\n",
      "Episode Reward: 0.0\n",
      "Step 10 (99999) @ Episode 414/10000, loss: 0.0005832621245644987\n",
      "Copied model parameters to target network.\n",
      "Step 303 (100292) @ Episode 414/10000, loss: 7.689121412113309e-058\n",
      "Episode Reward: 2.0\n",
      "Step 163 (100455) @ Episode 415/10000, loss: 0.00382532132789492652\n",
      "Episode Reward: 0.0\n",
      "Step 210 (100665) @ Episode 416/10000, loss: 0.00170437444467097526\n",
      "Episode Reward: 1.0\n",
      "Step 205 (100870) @ Episode 417/10000, loss: 0.00136573915369808677\n",
      "Episode Reward: 1.0\n",
      "Step 203 (101073) @ Episode 418/10000, loss: 0.00075132725760340693\n",
      "Episode Reward: 0.0\n",
      "Step 229 (101302) @ Episode 419/10000, loss: 0.00023348796821665028\n",
      "Episode Reward: 1.0\n",
      "Step 279 (101581) @ Episode 420/10000, loss: 0.00540829170495271757\n",
      "Episode Reward: 2.0\n",
      "Step 201 (101782) @ Episode 421/10000, loss: 0.00029417610494419937\n",
      "Episode Reward: 1.0\n",
      "Step 177 (101959) @ Episode 422/10000, loss: 0.00014677685976494104\n",
      "Episode Reward: 0.0\n",
      "Step 172 (102131) @ Episode 423/10000, loss: 0.00043561885831877592\n",
      "Episode Reward: 0.0\n",
      "Step 283 (102414) @ Episode 424/10000, loss: 0.00117381929885596042\n",
      "Episode Reward: 2.0\n",
      "Step 178 (102592) @ Episode 425/10000, loss: 7.48757302062586e-0586\n",
      "Episode Reward: 0.0\n",
      "Step 296 (102888) @ Episode 426/10000, loss: 0.00081769464304670697\n",
      "Episode Reward: 2.0\n",
      "Step 233 (103121) @ Episode 427/10000, loss: 0.00333988666534423834\n",
      "Episode Reward: 1.0\n",
      "Step 399 (103520) @ Episode 428/10000, loss: 7.496900798287243e-056\n",
      "Episode Reward: 4.0\n",
      "Step 250 (103770) @ Episode 429/10000, loss: 4.7798966988921165e-05\n",
      "Episode Reward: 2.0\n",
      "Step 174 (103944) @ Episode 430/10000, loss: 0.00155257212463766348\n",
      "Episode Reward: 0.0\n",
      "Step 208 (104152) @ Episode 431/10000, loss: 0.00024620274780318142\n",
      "Episode Reward: 1.0\n",
      "Step 166 (104318) @ Episode 432/10000, loss: 0.00036712182918563485\n",
      "Episode Reward: 0.0\n",
      "Step 172 (104490) @ Episode 433/10000, loss: 0.00037614823668263855\n",
      "Episode Reward: 0.0\n",
      "Step 304 (104794) @ Episode 434/10000, loss: 4.007441384601407e-056\n",
      "Episode Reward: 2.0\n",
      "Step 207 (105001) @ Episode 435/10000, loss: 0.00021578370069619268\n",
      "Episode Reward: 1.0\n",
      "Step 195 (105196) @ Episode 436/10000, loss: 0.00034779630368575454\n",
      "Episode Reward: 0.0\n",
      "Step 319 (105515) @ Episode 437/10000, loss: 0.00044666929170489314\n",
      "Episode Reward: 3.0\n",
      "Step 176 (105691) @ Episode 438/10000, loss: 0.00015423676813952625\n",
      "Episode Reward: 0.0\n",
      "Step 227 (105918) @ Episode 439/10000, loss: 0.48148205876350403266\n",
      "Episode Reward: 1.0\n",
      "Step 179 (106097) @ Episode 440/10000, loss: 0.00115983653813600546\n",
      "Episode Reward: 0.0\n",
      "Step 227 (106324) @ Episode 441/10000, loss: 6.669160211458802e-056\n",
      "Episode Reward: 1.0\n",
      "Step 169 (106493) @ Episode 442/10000, loss: 0.00041398860048502684\n",
      "Episode Reward: 0.0\n",
      "Step 138 (106631) @ Episode 443/10000, loss: 0.00055479741422459485"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-be11090fb7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dd1f41199636>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# Perform gradient descent update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mstates_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-49f3f6294684>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, y)\u001b[0m\n\u001b[1;32m     97\u001b[0m         summaries, global_step, _, loss = sess.run(\n\u001b[1;32m     98\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
