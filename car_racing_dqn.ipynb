{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc303/aer1517/car_racing_dqn/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "#env = gym.envs.make(\"Breakout-v0\")\n",
    "env = gym.make(\"CarRacing-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n",
    "# Racing Car a\n",
    "VALID_ACTIONS = np.arange(0,10)\n",
    "print(VALID_ACTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.2, 0.5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idx2act(num):\n",
    "    '''\n",
    "    convert action index to action input for racing-car-v0\n",
    "    \n",
    "    '''\n",
    "    steer = 0.0\n",
    "    gas = 0.2\n",
    "    brake = 0.0\n",
    "    if(num<7):\n",
    "        steer = (num-3)/3\n",
    "    if(num==7):\n",
    "        gas = 0.5\n",
    "    if(num==8):\n",
    "        gas = 1\n",
    "    if(num==9):\n",
    "        brake = 0.5\n",
    "        \n",
    "    return [steer,gas,brake]\n",
    "\n",
    "idx2act(9)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CR_StateProcessor():\n",
    "    def __init__(self):\n",
    "    # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[96, 96, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 0, 6, 84, 84)\n",
    "#             self.output = tf.image.resize_images(\n",
    "#                 self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CarRacing-v0\")\n",
    "# sp = CR_StateProcessor()\n",
    "# observation = env.reset()\n",
    "\n",
    "# for t in range(100):\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, done, _ = env.step(action)\n",
    "#     env.render()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     # Example observation batch\n",
    "#     #observation = env.reset()\n",
    "    \n",
    "#     observation_p = sp.process(sess, observation)\n",
    "#     print(observation_p)\n",
    "#     print(observation_p.shape)\n",
    "    \n",
    "#     plt.imshow(observation_p)\n",
    "#     plt.savefig(\"test.jpeg\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 grayscale frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Testing....\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "# e = Estimator(scope=\"test\")\n",
    "# sp = CR_StateProcessor()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     # Example observation batch\n",
    "#     observation = env.reset()\n",
    "    \n",
    "#     observation_p = sp.process(sess, observation)\n",
    "#     observation = np.stack([observation_p] * 4, axis=2)\n",
    "#     observations = np.array([observation] * 2)\n",
    "    \n",
    "#     # Test Prediction\n",
    "#     print(e.predict(sess, observations))\n",
    "\n",
    "#     # Test training step\n",
    "#     y = np.array([10.0, 10.0])\n",
    "#     a = np.array([1 ,9])\n",
    "#     print(e.update(sess, observations, a, y))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(idx2act(VALID_ACTIONS[action]))\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(idx2act(VALID_ACTIONS[action]))\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "            env.render()\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-49f3f6294684>:33: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-49f3f6294684>:59: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From /home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc303/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "Track generation: 1090..1366 -> 276-tiles track\n",
      "Track generation: 1120..1404 -> 284-tiles track\n",
      "Track generation: 1119..1410 -> 291-tiles track\n",
      "Track generation: 1169..1468 -> 299-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1208..1514 -> 306-tiles track\n",
      "Track generation: 1107..1388 -> 281-tiles track\n",
      "Track generation: 1252..1569 -> 317-tiles track\n",
      "Track generation: 1290..1624 -> 334-tiles track\n",
      "Track generation: 1128..1420 -> 292-tiles track\n",
      "Track generation: 1064..1341 -> 277-tiles track\n",
      "Track generation: 1038..1307 -> 269-tiles track\n",
      "Track generation: 1058..1335 -> 277-tiles track\n",
      "Track generation: 1088..1372 -> 284-tiles track\n",
      "Track generation: 1359..1703 -> 344-tiles track\n",
      "Track generation: 1193..1503 -> 310-tiles track\n",
      "Track generation: 1215..1523 -> 308-tiles track\n",
      "Track generation: 1329..1665 -> 336-tiles track\n",
      "Track generation: 1134..1425 -> 291-tiles track\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 334 (334) @ Episode 1/10000, loss: 0.0063622379675507545\n",
      "Episode Reward: -92.02068965517266\n",
      "Track generation: 1113..1395 -> 282-tiles track\n",
      "Step 580 (914) @ Episode 2/10000, loss: 0.0260334201157093055\n",
      "Episode Reward: -83.2669039145915\n",
      "Track generation: 1063..1319 -> 256-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Step 923 (1837) @ Episode 3/10000, loss: 1.3863432407379151795\n",
      "Episode Reward: -106.97576791808952\n",
      "Track generation: 1203..1508 -> 305-tiles track\n",
      "Step 999 (2836) @ Episode 4/10000, loss: 0.8347684741020203485\n",
      "Episode Reward: -34.210526315790254\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Step 437 (3273) @ Episode 5/10000, loss: 0.4428183436393738746\n",
      "Episode Reward: -75.20684931506901\n",
      "Track generation: 1219..1528 -> 309-tiles track\n",
      "Step 555 (3828) @ Episode 6/10000, loss: 0.336602032184600835\n",
      "Episode Reward: -80.82467532467606\n",
      "Track generation: 1119..1411 -> 292-tiles track\n",
      "Step 478 (4306) @ Episode 7/10000, loss: 0.326656997203826916\n",
      "Episode Reward: -61.88934707903836\n",
      "Track generation: 1134..1422 -> 288-tiles track\n",
      "Step 646 (4952) @ Episode 8/10000, loss: 0.546576559543609653\n",
      "Episode Reward: -112.33519163763106\n",
      "Track generation: 1211..1518 -> 307-tiles track\n",
      "Step 934 (5886) @ Episode 9/10000, loss: 0.4209219217300415175\n",
      "Episode Reward: -141.11241830065427\n",
      "Track generation: 1343..1683 -> 340-tiles track\n",
      "Step 999 (6885) @ Episode 10/10000, loss: 0.342561066150665348\n",
      "Episode Reward: 17.99410029498455\n",
      "Track generation: 1087..1363 -> 276-tiles track\n",
      "Step 999 (7884) @ Episode 11/10000, loss: 0.6184928417205815655\n",
      "Episode Reward: -9.090909090909285\n",
      "Track generation: 1197..1499 -> 302-tiles track\n",
      "Step 748 (8632) @ Episode 12/10000, loss: 0.643558084964752245\n",
      "Episode Reward: -105.03255813953552\n",
      "Track generation: 1290..1617 -> 327-tiles track\n",
      "Step 547 (9179) @ Episode 13/10000, loss: 0.0712084844708442756\n",
      "Episode Reward: -114.8226993865032\n",
      "Track generation: 1035..1303 -> 268-tiles track\n",
      "Step 387 (9566) @ Episode 14/10000, loss: 1.614025473594665559\n",
      "Episode Reward: 3.622097378278326\n",
      "Track generation: 1184..1484 -> 300-tiles track\n",
      "Step 347 (9913) @ Episode 15/10000, loss: 0.407011210918426575\n",
      "Episode Reward: -87.87725752508395\n",
      "Track generation: 1189..1490 -> 301-tiles track\n",
      "Step 86 (9999) @ Episode 16/10000, loss: 0.897279381752014276\n",
      "Copied model parameters to target network.\n",
      "Step 450 (10363) @ Episode 16/10000, loss: 0.182293847203254744\n",
      "Episode Reward: -98.33333333333363\n",
      "Track generation: 1040..1305 -> 265-tiles track\n",
      "Step 376 (10739) @ Episode 17/10000, loss: 0.343302249908447274\n",
      "Episode Reward: -88.35757575757611\n",
      "Track generation: 1080..1354 -> 274-tiles track\n",
      "Step 999 (11738) @ Episode 18/10000, loss: 0.365006893873214775\n",
      "Episode Reward: 57.509157509160865\n",
      "Track generation: 1282..1607 -> 325-tiles track\n",
      "Step 822 (12560) @ Episode 19/10000, loss: 0.437803030014038172\n",
      "Episode Reward: -77.26172839506285\n",
      "Track generation: 1223..1533 -> 310-tiles track\n",
      "Step 343 (12903) @ Episode 20/10000, loss: 0.249956309795379645\n",
      "Episode Reward: -95.46504854368949\n",
      "Track generation: 1251..1568 -> 317-tiles track\n",
      "Step 676 (13579) @ Episode 21/10000, loss: 0.351322263479232868\n",
      "Episode Reward: -91.65063291139315\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Step 614 (14193) @ Episode 22/10000, loss: 0.062145367264747627\n",
      "Episode Reward: -114.12727272727301\n",
      "Track generation: 1213..1520 -> 307-tiles track\n",
      "Step 444 (14637) @ Episode 23/10000, loss: 0.520440697669982953\n",
      "Episode Reward: -72.50457516339921\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Step 619 (15256) @ Episode 24/10000, loss: 0.224711999297142034\n",
      "Episode Reward: -78.86750902527118\n",
      "Track generation: 1084..1359 -> 275-tiles track\n",
      "Step 999 (16255) @ Episode 25/10000, loss: 0.5196657180786133485\n",
      "Episode Reward: 16.788321167881854\n",
      "Track generation: 1208..1514 -> 306-tiles track\n",
      "Step 612 (16867) @ Episode 26/10000, loss: 0.311837166547775274\n",
      "Episode Reward: -118.57704918032809\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "Step 999 (17866) @ Episode 27/10000, loss: 0.417213082313537657\n",
      "Episode Reward: -48.979591836735494\n",
      "Track generation: 1135..1429 -> 294-tiles track\n",
      "Step 340 (18206) @ Episode 28/10000, loss: 0.490191280841827485\n",
      "Episode Reward: -75.9795221843007\n",
      "Track generation: 1057..1333 -> 276-tiles track\n",
      "Step 724 (18930) @ Episode 29/10000, loss: 0.2197671085596084647\n",
      "Episode Reward: -59.67272727272587\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "Step 999 (19929) @ Episode 30/10000, loss: 0.0916111469268798843\n",
      "Episode Reward: 24.999999999998828\n",
      "Track generation: 994..1253 -> 259-tiles track\n",
      "Step 70 (19999) @ Episode 31/10000, loss: 0.091057002544403085\n",
      "Copied model parameters to target network.\n",
      "Step 563 (20492) @ Episode 31/10000, loss: 0.266161859035491947\n",
      "Episode Reward: -113.66434108527154\n",
      "Track generation: 1148..1439 -> 291-tiles track\n",
      "Step 550 (21042) @ Episode 32/10000, loss: 0.307539492845535355\n",
      "Episode Reward: -58.44827586206848\n",
      "Track generation: 1075..1348 -> 273-tiles track\n",
      "Step 999 (22041) @ Episode 33/10000, loss: 0.349133253097534225\n",
      "Episode Reward: -0.7352941176476584\n",
      "Track generation: 1109..1390 -> 281-tiles track\n",
      "Step 376 (22417) @ Episode 34/10000, loss: 0.270644545555114754\n",
      "Episode Reward: -59.0285714285718\n",
      "Track generation: 1107..1392 -> 285-tiles track\n",
      "Step 299 (22716) @ Episode 35/10000, loss: 0.123398989439010626\n",
      "Episode Reward: -77.08309859154964\n",
      "Track generation: 1257..1575 -> 318-tiles track\n",
      "Step 999 (23715) @ Episode 36/10000, loss: 0.240701109170913737\n",
      "Episode Reward: -46.37223974763467\n",
      "Track generation: 1078..1357 -> 279-tiles track\n",
      "Step 464 (24179) @ Episode 37/10000, loss: 0.506241440773010348\n",
      "Episode Reward: -103.23453237410101\n",
      "Track generation: 1087..1364 -> 277-tiles track\n",
      "Step 999 (25178) @ Episode 38/10000, loss: 0.108579367399215745\n",
      "Episode Reward: -20.28985507246458\n",
      "Track generation: 1091..1368 -> 277-tiles track\n",
      "Step 999 (26177) @ Episode 39/10000, loss: 0.058870080858469012\n",
      "Episode Reward: -5.797101449276065\n",
      "Track generation: 1198..1501 -> 303-tiles track\n",
      "Step 667 (26844) @ Episode 40/10000, loss: 0.280060052871704185\n",
      "Episode Reward: -120.34238410596058\n",
      "Track generation: 1060..1329 -> 269-tiles track\n",
      "Step 331 (27175) @ Episode 41/10000, loss: 0.167452260851860055\n",
      "Episode Reward: -88.32388059701525\n",
      "Track generation: 1236..1549 -> 313-tiles track\n",
      "Step 999 (28174) @ Episode 42/10000, loss: 0.317810654640197752\n",
      "Episode Reward: -3.8461538461544587\n",
      "Track generation: 1111..1397 -> 286-tiles track\n",
      "Step 371 (28545) @ Episode 43/10000, loss: 0.275380879640579276\n",
      "Episode Reward: -105.52105263157905\n",
      "Track generation: 1166..1462 -> 296-tiles track\n",
      "Step 999 (29544) @ Episode 44/10000, loss: 0.133348271250724895\n",
      "Episode Reward: -42.37288135593275\n",
      "Track generation: 1051..1316 -> 265-tiles track\n",
      "Step 455 (29999) @ Episode 45/10000, loss: 0.194143652915954653\n",
      "Copied model parameters to target network.\n",
      "Step 693 (30237) @ Episode 45/10000, loss: 0.307734489440917976\n",
      "Episode Reward: -67.02727272727213\n",
      "Track generation: 1078..1361 -> 283-tiles track\n",
      "Step 999 (31236) @ Episode 46/10000, loss: 0.062769860029220585\n",
      "Episode Reward: -25.53191489361785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1150..1445 -> 295-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1083..1358 -> 275-tiles track\n",
      "Step 999 (32235) @ Episode 47/10000, loss: 0.055320091545581824\n",
      "Episode Reward: -19.708029197081014\n",
      "Track generation: 1214..1522 -> 308-tiles track\n",
      "Step 999 (33234) @ Episode 48/10000, loss: 0.301598995923995974\n",
      "Episode Reward: 17.263843648207093\n",
      "Track generation: 1200..1504 -> 304-tiles track\n",
      "Step 270 (33504) @ Episode 49/10000, loss: 0.310743421316146852\n",
      "Episode Reward: -80.79537953795409\n",
      "Track generation: 1175..1473 -> 298-tiles track\n",
      "Step 999 (34503) @ Episode 50/10000, loss: 0.095764182507991796\n",
      "Episode Reward: 4.37710437710367\n",
      "Track generation: 996..1249 -> 253-tiles track\n",
      "Step 999 (35502) @ Episode 51/10000, loss: 0.248950690031051646\n",
      "Episode Reward: 161.90476190476417\n",
      "Track generation: 1111..1393 -> 282-tiles track\n",
      "Step 964 (36466) @ Episode 52/10000, loss: 0.238874301314353945\n",
      "Episode Reward: -86.07971530249236\n",
      "Track generation: 1352..1693 -> 341-tiles track\n",
      "Step 905 (37371) @ Episode 53/10000, loss: 312.6583557128906787\n",
      "Episode Reward: -93.44117647058877\n",
      "Track generation: 1144..1434 -> 290-tiles track\n",
      "Step 999 (38370) @ Episode 54/10000, loss: 0.091796912252902986\n",
      "Episode Reward: -41.176470588236086\n",
      "Track generation: 1299..1628 -> 329-tiles track\n",
      "Step 521 (38891) @ Episode 55/10000, loss: 0.034860797226428986\n",
      "Episode Reward: -81.97804878048845\n",
      "Track generation: 1375..1723 -> 348-tiles track\n",
      "Step 999 (39890) @ Episode 56/10000, loss: 0.098058529198169716\n",
      "Episode Reward: -16.426512968300408\n",
      "Track generation: 1220..1529 -> 309-tiles track\n",
      "Step 109 (39999) @ Episode 57/10000, loss: 0.205111920833587656\n",
      "Copied model parameters to target network.\n",
      "Step 374 (40264) @ Episode 57/10000, loss: 0.258119046688079834\n",
      "Episode Reward: -91.94545454545485\n",
      "Track generation: 1231..1553 -> 322-tiles track\n",
      "Step 390 (40654) @ Episode 58/10000, loss: 0.133526146411895756\n",
      "Episode Reward: -67.34890965732129\n",
      "Track generation: 1324..1661 -> 337-tiles track\n",
      "Step 795 (41449) @ Episode 59/10000, loss: 0.246536970138549857\n",
      "Episode Reward: -119.97619047619096\n",
      "Track generation: 1113..1399 -> 286-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 897..1129 -> 232-tiles track\n",
      "Step 673 (42122) @ Episode 60/10000, loss: 0.038902476429939276\n",
      "Episode Reward: -93.70692640692715\n",
      "Track generation: 1093..1370 -> 277-tiles track\n",
      "Step 318 (42440) @ Episode 61/10000, loss: 0.239935770630836584\n",
      "Episode Reward: -62.959420289855444\n",
      "Track generation: 1274..1596 -> 322-tiles track\n",
      "Step 999 (43439) @ Episode 62/10000, loss: 0.107409730553627015\n",
      "Episode Reward: -65.7320872274149\n",
      "Track generation: 1091..1372 -> 281-tiles track\n",
      "Step 388 (43827) @ Episode 63/10000, loss: 0.026792794466018677\n",
      "Episode Reward: -103.08571428571445\n",
      "Track generation: 1032..1297 -> 265-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1248..1564 -> 316-tiles track\n",
      "Step 999 (44826) @ Episode 64/10000, loss: 0.221185654401779174\n",
      "Episode Reward: -42.85714285714336\n",
      "Track generation: 1414..1775 -> 361-tiles track\n",
      "Step 999 (45825) @ Episode 65/10000, loss: 0.121423810720443736\n",
      "Episode Reward: 11.111111111110477\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Step 999 (46824) @ Episode 66/10000, loss: 0.061933960765600204\n",
      "Episode Reward: 78.45117845117774\n",
      "Track generation: 1195..1498 -> 303-tiles track\n",
      "Step 964 (47788) @ Episode 67/10000, loss: 0.061254851520061492\n",
      "Episode Reward: -97.0622516556301\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Step 947 (48735) @ Episode 68/10000, loss: 0.125675618648529057\n",
      "Episode Reward: -128.69339933993473\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Step 999 (49734) @ Episode 69/10000, loss: 0.119995072484016429\n",
      "Episode Reward: 4.529616724737846\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Step 265 (49999) @ Episode 70/10000, loss: 0.204773351550102234\n",
      "Copied model parameters to target network.\n",
      "Step 999 (50733) @ Episode 70/10000, loss: 0.314979791641235354\n",
      "Episode Reward: -9.747292418773041\n",
      "Track generation: 1244..1559 -> 315-tiles track\n",
      "Step 805 (51538) @ Episode 71/10000, loss: 0.187548071146011356\n",
      "Episode Reward: -113.62101910828079\n",
      "Track generation: 959..1203 -> 244-tiles track\n",
      "Step 999 (52537) @ Episode 72/10000, loss: 0.025134434923529625\n",
      "Episode Reward: 85.18518518518685\n",
      "Track generation: 1232..1545 -> 313-tiles track\n",
      "Step 999 (53536) @ Episode 73/10000, loss: 0.155245363712310853\n",
      "Episode Reward: 34.615384615386034\n",
      "Track generation: 1107..1396 -> 289-tiles track\n",
      "Step 700 (54236) @ Episode 74/10000, loss: 0.073522940278053284\n",
      "Episode Reward: -107.50000000000054\n",
      "Track generation: 1047..1316 -> 269-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1251..1568 -> 317-tiles track\n",
      "Step 999 (55235) @ Episode 75/10000, loss: 0.300253450870513914\n",
      "Episode Reward: 4.430379746834891\n",
      "Track generation: 1117..1400 -> 283-tiles track\n",
      "Step 653 (55888) @ Episode 76/10000, loss: 0.195860296487808236\n",
      "Episode Reward: -105.01631205673812\n",
      "Track generation: 1201..1505 -> 304-tiles track\n",
      "Step 999 (56887) @ Episode 77/10000, loss: 0.081835024058818824\n",
      "Episode Reward: -33.9933993399348\n",
      "Track generation: 1235..1548 -> 313-tiles track\n",
      "Step 906 (57793) @ Episode 78/10000, loss: 0.104950301349163064\n",
      "Episode Reward: -126.49743589743635\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Step 999 (58792) @ Episode 79/10000, loss: 0.111428573727607736\n",
      "Episode Reward: 10.320284697507669\n",
      "Track generation: 1115..1401 -> 286-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1109..1390 -> 281-tiles track\n",
      "Step 999 (59791) @ Episode 80/10000, loss: 0.028901973739266396\n",
      "Episode Reward: -7.142857142858118\n",
      "Track generation: 1173..1470 -> 297-tiles track\n",
      "Step 208 (59999) @ Episode 81/10000, loss: 0.194602280855178835\n",
      "Copied model parameters to target network.\n",
      "Step 999 (60790) @ Episode 81/10000, loss: 0.311816662549972534\n",
      "Episode Reward: -45.94594594594643\n",
      "Track generation: 1224..1534 -> 310-tiles track\n",
      "Step 999 (61789) @ Episode 82/10000, loss: 0.140923425555229275\n",
      "Episode Reward: -12.621359223301514\n",
      "Track generation: 1187..1488 -> 301-tiles track\n",
      "Step 999 (62788) @ Episode 83/10000, loss: 0.102110758423805246\n",
      "Episode Reward: -13.333333333333995\n",
      "Track generation: 1215..1523 -> 308-tiles track\n",
      "Step 199 (62987) @ Episode 84/10000, loss: 0.148088365793228154"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-10379c2adaab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9119f0d4d711>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# Perform gradient descent update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mstates_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-49f3f6294684>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, y)\u001b[0m\n\u001b[1;32m     97\u001b[0m         summaries, global_step, _, loss = sess.run(\n\u001b[1;32m     98\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aer1517/car_racing_dqn/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = CR_StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=10000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
